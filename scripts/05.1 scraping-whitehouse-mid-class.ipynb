{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üéì **Professor**: Apostolos Filippas\n",
    "\n",
    "### üìò **Class**: Web Analytics\n",
    "\n",
    "### üìã **Topic**: Scraping data using Beautiful Soup\n",
    "\n",
    "### üîó **Link**: ‚Äã‚Äã[https://bit.ly/WA_LEC5](‚Äã‚Äãhttps://bit.ly/WA_LEC5)\n",
    "\n",
    "\n",
    "üö´ **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---\n",
    "\n",
    "# üåê 0. Introduction\n",
    "\n",
    "This notebook details how we can scrape whitehouse statement text, types, and links, with dates.\n",
    "\n",
    "As always, we will begin by importing our libraries.\n",
    "\n",
    "Let's also create \n",
    "- `data`: an empty list, where we will be appending our content\n",
    "- `headers`: a dictionary that tells our requests as what kind of browser we are requesting the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. importing useful libraries\n",
    "\n",
    "import requests # For making HTTP requests to the web.\n",
    "import time     # To introduce pauses in our code, ensuring we don't overwhelm the server.\n",
    "import re       # To help us extract specific patterns from text.\n",
    "from bs4 import BeautifulSoup # A popular library to parse and navigate HTML content\n",
    "\n",
    "# create an empty list to store our scraped data\n",
    "data  = [] \n",
    "# Define headers to simulate a browser request. \n",
    "# This can help bypass restrictions that prevent scripts or bots from accessing web content.\n",
    "my_headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìï 1. Fetching the Webpage\n",
    "\n",
    "First, we will try to get the page five times:\n",
    "- ‚úÖ **Success**: if we are successful, we will immediately exit the loop\n",
    "- üö´ **Failure**: if we are unsuccessful, the `requests.get` method will trigger an error. As a response, the code will print a message indicating this\n",
    "\n",
    "At then end, we will tell the user whether we got the page or not\n",
    "- try to get a page that does not exist, e.g. https://www.whitsadsadasdasdehouse.gov/briefing-room/page/1/' and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target URL\n",
    "page = 'https://www.whitehouse.gov/briefing-room/page/1/' \n",
    "\n",
    "# Initialize the source content to None \n",
    "# (False would work as well, but using None is more Python-idiomatic)\n",
    "src = None\n",
    "\n",
    "# Now get the page\n",
    "\n",
    "# Try to scrape the page up to 5 times\n",
    "for i in range(5): \n",
    "    try:\n",
    "        # Fetch the content of the URL with the specified headers\n",
    "        response = requests.get(page, headers = my_headers)\n",
    "        # If the request was successful, store the page content in src & exit the loop\n",
    "        src = response.content\n",
    "        break \n",
    "    # Catch specific exception for request.get() errors\n",
    "    except:\n",
    "        print (f'Failed attempt # {i+1}')\n",
    "        # wait 2 secs before trying again\n",
    "        time.sleep(2)\n",
    "\n",
    "# Let the user know if the page content was fetched successfully\n",
    "if src:\n",
    "   print(f'Successfully got page: {page}')\n",
    "else:\n",
    "   print(f'Could not get page: {page}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üî¢ 2. Encoding\n",
    "\n",
    "When we fetch data from the web using libraries like requests, the data we get can be in various formats such as HTML, JSON, images, etc. This raw data is sent over the internet in the form of bytes. Therefore, when we receive the data, it's in its raw byte form. \n",
    "\n",
    "Bytes are represented in Python as a `bytes` object.\n",
    "- The bytes object in Python is an immutable sequence of bytes used to represent raw, binary data.\n",
    "- \"Bytes\" are the smallest addressable unit in a computer and can represent a wide variety of data, including text, images, audio, and more.\n",
    "\n",
    "`bytes` are different from `strings`\n",
    "-  Strings in Python represent text, and each character in a string corresponds to a specific symbol. When we want to store or transmit this text, it needs to be encoded into bytes, which is where encodings like ASCII, utf-8, and others come into play. \n",
    "-  Each encoding has its own way of mapping characters to sequences of bytes.\n",
    "-  Strings are sequences of characters, while bytes are sequences of bytes.\n",
    "-  Conversion between strings and bytes is done via encoding (to bytes) and decoding (to string):\n",
    "\n",
    "To decode bytes to strings, we need to know how the bytes were encoded.\n",
    "- The server tells us the encoding of the data it sends us in the response headers.\n",
    "- requests also comes with a built-in encoding detector, which guesses the encoding of the content it receives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src)\n",
    "print(\"Encoding as per headers:\", response.encoding)\n",
    "print(\"Apparent encoding:\", response.apparent_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# üçú 3. Using Beautiful Soup to parse the information\n",
    "\n",
    "Beautiful Soup is a Python library designed for web scraping purposes to pull the data out of HTML and XML files. It creates parse trees from these files that are helpful to extract the data easily.\n",
    "\n",
    "## 3.1 Parsing the HTML as a Beautiful Soup object\n",
    "Before we parse the content, we need to ensure that it is in a suitable encoding. While the internet contains data in a plethora of encodings, ASCII is one of the most basic. By decoding to ASCII, we're simplifying the content, but it's worth noting that we may lose special characters or symbols that are not represented in ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BeautifulSoup object with the BeautifulSoup constructor\n",
    "soup = BeautifulSoup(src.decode('utf-8', 'ignore'), 'html.parser')\n",
    "# confirm that you have successfully created a BeautifulSoup object\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why `decode('ascii', 'ignore')`?\n",
    "- **decode('ascii')**: This converts bytes (which is the type of our fetched content) into a string using the 'utf-8' encoding.\n",
    "- **'ignore' parameter**: If characters in the bytes aren't valid in 'utf-8', instead of throwing an error, the 'ignore' option tells Python to just skip over them. This is especially useful when scraping web content as we can sometimes encounter non-standard characters.\n",
    "\n",
    "The `BeautifulSoup()` constructor needs two arguments:\n",
    "- **The HTML (or XML) content** that we are going to parse.\n",
    "- **The parser library name.** In this case, we're using the `html.parser`, which is a nice built-in parser. There are other parsers, such as `html5lib`, and `lxml`. Often, the `lxml` parser is preferred for its speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Parse Tree\n",
    "\n",
    "A parse tree, often referred to as a *parsing tree* or *syntax tree*, represents syntactic constructs in source code or any structured text. It's utilized to portray the code or content's structure in a hierarchical format.\n",
    "\n",
    "For HTML and XML documents, the parse tree denotes the structure of the document. Each tag, attribute, and piece of text stands as a node in this tree.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Node**: Each unique part of the tree, such as an HTML tag, is termed a node. For instance, in an HTML document like `<title>My Web Page</title>`, both the `<title>` tag and the text \"My Web Page\" are nodes.\n",
    "\n",
    "- **Root Node**: This is the highest node in the tree. In the context of an HTML document, the root node is typically represented by the `<html>` tag.\n",
    "\n",
    "- **Child Nodes**: These are nodes that stem from another node. For HTML, tags and content nested within another tag are its child nodes. As an example, in `<body><p>Text</p></body>`, `<p>` is a child node of `<body>`.\n",
    "\n",
    "- **Parent Node**: The opposite of child nodes. If node A is a descendant of node B, node B is the progenitor of node A.\n",
    "\n",
    "- **Sibling Nodes**: These nodes share the same parent. If two tags or textual pieces are housed within the same parent tag, they are acknowledged as siblings.\n",
    "\n",
    "### Why is the Parse Tree Important?\n",
    "\n",
    "- **Navigation**: With the HTML or XML content transformed into a parse tree, navigation becomes possible. This empowers users to discover specific document segments, traverse the structure in various directions, and pull out the desired data.\n",
    "\n",
    "- **Search**: Specific tags, classes, IDs, or other attributes can be located. This capability is a cornerstone in web scraping.\n",
    "\n",
    "- **Modification**: Content can be altered through the parse tree. This includes the addition, removal, or alteration of tags and attributes.\n",
    "\n",
    "- **Extraction**: Once the desired data is pinpointed, structured extraction becomes feasible.\n",
    "\n",
    "### Example\n",
    "Here's some HTML\n",
    "```\n",
    "<html>\n",
    "    <head>\n",
    "        <title>My Web Page</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"intro\">Welcome to my web page!</p>\n",
    "        <p>Here's some more text.</p>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Parse Tree Structure:\n",
    "- `<html>` (Root Node)\n",
    "  - `<head>` (Child of `<html>`)\n",
    "    - `<title>` (Child of `<head>`)\n",
    "      - \"My Web Page\" (Text node, child of `<title>`)\n",
    "  - `<body>` (Child of `<html>`, sibling of `<head>`)\n",
    "    - `<p class=\"intro\">` (Child of <body>)\n",
    "      - \"Welcome to my web page!\" (Text node, child of first `<p>`)\n",
    "    - `<p>` (Second `<p>` tag, child of `<body>`, sibling of the first `<p>`)\n",
    "      - \"Here's some more text.\" (Text node, child of second `<p>`)\n",
    "\n",
    "The representation above is a basic idea of how the parse tree would look for the given HTML.\n",
    "\n",
    "### Beautiful Soup and the Parse Tree:\n",
    "\n",
    "When Beautiful Soup is employed to dissect an HTML or XML document, it creates an in-memory parse tree from the page source code. This grants the ability to engage with the page's architecture, navigate, seek out, and alter its components. Functions such as \n",
    "- `find()`, \n",
    "- `find_all()`, \n",
    "- `parent`, \n",
    "- `children`, \n",
    "- `next_sibling`, \n",
    "- `previous_sibling`, \n",
    "\n",
    "and more, facilitate movement and interaction within this tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Using find_all with Beautiful Soup\n",
    "The `find_all` method is one of the most commonly used methods in Beautiful Soup to search the parse tree. It returns all the matching tags found in the document, in the form of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all(\"a\")\n",
    "# observe the similarity with  re.findall('<a .+</a>', response.text ) \n",
    "print(len(links))\n",
    "links[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above line, `soup.find_all(\"a\")` looks for all `<a>` tags, commonly used for hyperlinks, in the parsed HTML document. It's equivalent to using a regular expression like `re.findall('<a .+</a>', response.text)` which attempts to find all patterns in the raw HTML that resemble anchor tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = links[16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(element.parent.parent.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in element.children:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç 4. Finding Data Location using the \"Inspect\" tool\n",
    "\n",
    "\n",
    "To accurately extract information, it's crucial to first identify where it resides within the webpage's structure. For each statement, we aim to retrieve details on: statement type, link text and link address\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"http://drive.google.com/uc?export=view&id=1z6BUBD1JvKI00-1zr5VXRvLHFOJuGRkA\" width=\"600\">\n",
    "    <br><br>\n",
    "</div>\n",
    "\n",
    "Using your browser's \"inspect\" feature expedites the scraping process:\n",
    "\n",
    "1. **Initiate Inspect Mode**: Right-click on any webpage element and select \"Inspect\".\n",
    "2. **Element Selector**: Click the \"Select an element\" tool.\n",
    "3. **Element Navigation**: Hover over the webpage elements. The corresponding HTML structure will be highlighted. Click when you've located the desired element to pin its HTML code in the inspector.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"http://drive.google.com/uc?export=view&id=17MUJ6s-TA8zRfdwk8EmEb9UgMwQyAsCl\" width=\"600\">\n",
    "    <br><br>\n",
    "</div>\n",
    "\n",
    "Using the inspect tool allows you to find the HTML segments containing the data you want to access.\n",
    "\n",
    "### üåç **Google Chrome**:\n",
    "1. **Right-click** on an element.\n",
    "2. Select **\"Inspect\"** from the context menu.\n",
    "3. Alternatively, use the shortcut **Ctrl + Shift + I** (Windows/Linux) or **Cmd + Option + I** (Mac).\n",
    "\n",
    "### ü¶ä **Mozilla Firefox**:\n",
    "1. **Right-click** on an element.\n",
    "2. Choose **\"Inspect Element\"** from the dropdown.\n",
    "3. Shortcut: **Ctrl + Shift + C** (Windows/Linux) or **Cmd + Option + C** (Mac).\n",
    "\n",
    "### üåê **Microsoft Edge**:\n",
    "1. **Right-click** on the desired item.\n",
    "2. Opt for **\"Inspect Element\"**.\n",
    "3. You can also use **F12** or **Ctrl + Shift + I** to directly open the developer tools.\n",
    "\n",
    "### üçè **Safari**:\n",
    "(Note: You might need to enable the \"Develop\" menu first from Preferences > Advanced)\n",
    "1. **Right-click** on the element (if you have a Mac with a single button mouse, Ctrl + click).\n",
    "2. Choose **\"Inspect Element\"**.\n",
    "3. Shortcut: **Cmd + Option + C**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ 5. Getting what we want\n",
    "\n",
    "## 5.1 Finding a tag\n",
    "\n",
    "Next, we need to find what is the tag that \"contains\" the information we want. \n",
    "\n",
    "Inspecting the HTML we find that the tag `<article>` with an attribute of type `class` which has value \"news-item__title-container\" contains tags with the info that we want to parse...: \n",
    "1. the `<a>` tag with with an attribute of type `class` which has value \"news-item__title\" \n",
    "2. ... and other information\n",
    "\n",
    "By saying \"contain\" we mean that this information is found after the article tag opens, and before it closes.\n",
    "- alternatively, we are looking at the \"children\" of the `<article>` tag\n",
    "\n",
    "This is easy to see in the \"inspect view\" because everything that an element contains, has more indent than that element!!\n",
    "\n",
    "See for example below:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"http://drive.google.com/uc?export=view&id=1OPDAOzhQBBEuqR7kc0y_fBTkpfIqs2z8\" width=\"600\">\n",
    "    <br><br>\n",
    "</div>\n",
    "\n",
    "To find all article tags, we simply ask Beautiful Soup to look for this tag in the \"soup\" variable---that contains the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all('article')\n",
    "articles[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now verify that the first element of the articles list contains an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also print an easier-to-see version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles[0].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at the attributes of that html tag, and veryify that they are the tags that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[0].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Let's say you have the following HTML:\n",
    "```\n",
    "<div>\n",
    "    <a href=\"#\">Link 1</a>\n",
    "    <div>\n",
    "        <a href=\"#\">Nested Link 1</a>\n",
    "        <a href=\"#\">Nested Link 2</a>\n",
    "    </div>\n",
    "</div>\n",
    "```\n",
    "If you use `soup.find_all(\"a\")`, it will return all three `<a>` tags: \"Link 1\", \"Nested Link 1\", and \"Nested Link 2\". Even though \"Nested Link 1\" and \"Nested Link 2\" are inside a nested `<div>`, they are still returned as separate matches.\n",
    "\n",
    "\n",
    "\n",
    "## 5.2 Keeping the right tags\n",
    "As articles may be used for other kind of information, we would like to keep only the articles with the right attributes. The way to do this is by telling Beautiful Soup to only keep the articles who are briefing statements, which is done as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only <article> tags whose class contains the \"news-item\" substring\n",
    "articles = soup.find_all('article', {'class':'news-item'})\n",
    "\n",
    "# we can also write this as follows\n",
    "# note: we need the underscore because \"class\" is a reserved word in Python \n",
    "articles = soup.find_all('article', class_='news-item')\n",
    "\n",
    "# we can also write this as follows\n",
    "# this allows us to use regular expressions to match the class name\n",
    "articles = soup.find_all('article', {'class':re.compile('news-item')})\n",
    "\n",
    "\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have all articles, let's grab the correct information for each article.\n",
    "We need to grab two kinds of information:\n",
    "    \n",
    "- the text that is contained within the **p** tag with attribute \"class\" of value \"briefing-statement__type\"\n",
    "- (i) the value of the \"href\" attribute and (ii) the text of the **a** tag _that is contained_ within the **h2** tag with attribute \"class\" of value \"briefing-statement__title\"\n",
    "\n",
    "## 5.3 Getting the text\n",
    "Let's first do this with the first article, and then do it for every article using a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = articles[0]\n",
    "print(article.prettify())\n",
    "print(type(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first grab the <a> tag and everythign it contains from the first article\n",
    "# to do so, simply use beautiful soups \"find\" functionality \n",
    "# (because there is only one, it will return the first occurence)\n",
    "a = article.find('a')\n",
    "print(a.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the text from the h2 article simply using the .text function of beautiful soup\n",
    "article_text = a.get_text()\n",
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the text from unecessary whitespace using the native BS2 function\n",
    "article_text = a.get_text(strip=True)\n",
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the text from unecessary whitespace using Python\n",
    "article_text = a.get_text().strip().replace('\\xa0', ' ')\n",
    "article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Getting the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_url = a.attrs.get('href')\n",
    "article_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'url   = {article_url}')\n",
    "print(f'text  = {article_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Getting information for all articles\n",
    "Now to scrape all articles from the webpage, simply use a for loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "articles = soup.findAll('article', {'class':'news-item'})\n",
    "\n",
    "for article in articles:\n",
    "    \n",
    "    # find a, grab text, strip() it\n",
    "    a = article.find('a')\n",
    "    article_text = a.get_text(strip=True)\n",
    "    \n",
    "    # and get the url too\n",
    "    article_url = a.attrs.get('href')\n",
    "    \n",
    "    # add all the info to the data link\n",
    "    data.append([article_url, article_text])\n",
    "\n",
    "for article in data:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pro version\n",
    "articles = soup.findAll('article', {'class':'news-item'})\n",
    "\n",
    "data = [\n",
    "    [a.attrs.get('href'), a.get_text(strip=True)]\n",
    "    for article in articles if (a := article.find('a'))\n",
    "]\n",
    "\n",
    "for url, text in data:\n",
    "    print(f\"URL: {url}\\nText: {text}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Scraping all articles in webpage with failsafes\n",
    "Let's now see how to deal with cases where we search for elements that do not exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "articles = soup.findAll('article', {'class':'news-item'})\n",
    "\n",
    "for article in articles:\n",
    "    \n",
    "    # initialize to not found\n",
    "    article_url  = None\n",
    "    article_text = None\n",
    "    \n",
    "    # find p, grab text, strip() it\n",
    "    a = article.find('a')\n",
    "    \n",
    "    # if you found it\n",
    "    if a:\n",
    "        article_text =  a.get_text(strip=True)\n",
    "        article_url  =  a.attrs.get('href')\n",
    "    \n",
    "    # add all the info to the data link\n",
    "    # if some element hasn't been found, the 'NA' string will be added\n",
    "    data.append([article_url, article_text])\n",
    "\n",
    "for article in data:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÉ 6. Scraping many pages!\n",
    "\n",
    "To scrape many pages, we simply add a for loop to everything...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "numPages = 5\n",
    "\n",
    "for k in range(1,numPages+1):\n",
    "    \n",
    "    # Give the url of the page\n",
    "    page = f'https://www.whitehouse.gov/briefing-room/page/{k}/' \n",
    "    # Initialize src to be False\n",
    "    src  = None\n",
    "\n",
    "    # Now get the page\n",
    "\n",
    "    # try to scrape 5 times\n",
    "    for i in range(1,6): \n",
    "        try:\n",
    "            # get url content\n",
    "            response = requests.get(page, headers = my_headers)\n",
    "            # get the html content\n",
    "            src = response.content\n",
    "            # if we successuflly got the file, break the loop\n",
    "            break \n",
    "        # if requests.get() threw an exception, i.e., the attempt to get the response failed\n",
    "        except:\n",
    "            print (f'failed attempt # {i}')\n",
    "            # wait 2 secs before trying again\n",
    "            time.sleep(2)\n",
    "\n",
    "    # if successful, let the user now\n",
    "    if src:\n",
    "       print(f'Successfully got page: {page}')\n",
    "    # if unsuccessful, notify the user and move to the next page\n",
    "    else:\n",
    "       print('Could not get page: {page}')\n",
    "       continue \n",
    "    \n",
    "    soup     = BeautifulSoup(src.decode('utf-8', 'ignore'), 'html.parser')\n",
    "    articles = soup.findAll('article', {'class':'news-item'})\n",
    "\n",
    "    for article in articles:\n",
    "\n",
    "        # initialize to not found\n",
    "        article_url  = None\n",
    "        article_text = None\n",
    "\n",
    "        # find p, grab text, strip() it\n",
    "        a = article.find('a')\n",
    "\n",
    "        # if you found it\n",
    "        if a:\n",
    "            article_text =  a.get_text(strip=True)\n",
    "            article_url  =  a.attrs.get('href')\n",
    "\n",
    "        # add all the info to the data link\n",
    "        # if some element hasn't been found, the 'NA' string will be added\n",
    "        data.append([article_url, article_text])\n",
    "    \n",
    "    # always a good idea to take a nap\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in data:\n",
    "        print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color='red'>CHALLENGE</font></center>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"http://drive.google.com/uc?export=view&id=1Vn93aS3wwZjatSJlYlggO6XT7cykEG9w\" width=\"500\">\n",
    "    <br><br>\n",
    "</div>\n",
    "\n",
    "In the remainder of the class I ask that you work on the following challenge (+1% bonus)\n",
    "\n",
    "1. Extend today's code to also scrape the \"type\" and \"date\" of each announcement\n",
    "2. Save all information that you scraped about each announcement in a .txt file. Each announcement on the website should be a line in the file, and each article's attributes (title, url, type,data) should be tab-separated , like **Article_title\\tArticle_url\\tArticle_date\\tArticle_type\\n**\n",
    "3. After you've saved the data, make sure you can read it using Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "numPages = 5\n",
    "\n",
    "for k in range(1,numPages+1):\n",
    "    \n",
    "    # Give the url of the page\n",
    "    page = f'https://www.whitehouse.gov/briefing-room/page/{k}/' \n",
    "    # Initialize src to be False\n",
    "    src  = None\n",
    "\n",
    "    # Now get the page\n",
    "\n",
    "    # try to scrape 5 times\n",
    "    for i in range(1,6): \n",
    "        try:\n",
    "            # get url content\n",
    "            response = requests.get(page, headers = my_headers)\n",
    "            # get the html content\n",
    "            src = response.content\n",
    "            # if we successuflly got the file, break the loop\n",
    "            break \n",
    "        # if requests.get() threw an exception, i.e., the attempt to get the response failed\n",
    "        except:\n",
    "            print (f'failed attempt # {i}')\n",
    "            # wait 2 secs before trying again\n",
    "            time.sleep(2)\n",
    "\n",
    "    # if successful, let the user now\n",
    "    if src:\n",
    "       print(f'Successfully got page: {page}')\n",
    "    # if unsuccessful, notify the user and move to the next page\n",
    "    else:\n",
    "       print('Could not get page: {page}')\n",
    "       continue \n",
    "    \n",
    "    soup     = BeautifulSoup(src.decode('utf-8', 'ignore'), 'html.parser')\n",
    "    articles = soup.findAll('article', {'class':'news-item'})\n",
    "\n",
    "    for article in articles:\n",
    "\n",
    "        # initialize to not found\n",
    "        article_url  = None\n",
    "        article_text = None\n",
    "        article_date = None # <-- you have to retrieve this! -->\n",
    "        article_type = None # <-- you have to retrieve this! --> \n",
    "\n",
    "        # find p, grab text, strip() it\n",
    "        a = article.find('a')\n",
    "\n",
    "        # if you found it\n",
    "        if a:\n",
    "            article_text =  a.get_text(strip=True)\n",
    "            article_url  =  a.attrs.get('href')\n",
    "\n",
    "        # add all the info to the data link\n",
    "        # if some element hasn't been found, the None value will be added\n",
    "        # <-- you have to store the info you retrieved -->\n",
    "        data.append([article_url, article_text])\n",
    "    \n",
    "    # always a good idea to take a nap\n",
    "    time.sleep(2)\n",
    "\n",
    "# <-- you have to save your data! -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
