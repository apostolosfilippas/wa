{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLZALWWZcvOQ"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: Web Analytics\n",
    "\n",
    "### ðŸ“‹ **Topic**: Pandas (self-study)\n",
    "\n",
    "### ðŸ”— **Link**: bit.ly/WA_LEC7_LINEAR\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLi-ggOTcvOR"
   },
   "source": [
    "# ðŸª„ 1. Learning a Linear Model\n",
    "\n",
    "Much of the power of data comes from it's ability to help us predict unknown (e.g., future) quantities. \n",
    "\n",
    "## 1.1 Getting and cleaning our data\n",
    "Let's build a simple model to predict the mpg of cars from the other information we have available on those cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9XhvMpccvOR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# url from which to retrieve the data (UCI Machine Learning Repository)\n",
    "# more info on the dataset: https://archive.ics.uci.edu/dataset/9/auto+mpg\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original\"\n",
    "\n",
    "# define the names of the columns of our data\n",
    "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration',\n",
    "                'model', 'origin', 'car_name']\n",
    "\n",
    "# read the data from the url into a pandas dataframe\n",
    "mpg_df = pd.read_csv(url,\n",
    "                     delim_whitespace=True,\n",
    "                     header=None,\n",
    "                     names=column_names)\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "mpg_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr08mqBIcvOS"
   },
   "source": [
    "First let's separate our X (attributes) from our y (target variable, attribute to be predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ulZ9Cq1IcvOT"
   },
   "outputs": [],
   "source": [
    "# we'll use these columns as the features we will use in our predictions\n",
    "predictors = [\"weight\", \"acceleration\", \"horsepower\", \"cylinders\", \"displacement\"]\n",
    "# and this column as a target\n",
    "target = \"mpg\"\n",
    "\n",
    "# drop any NaNs for now\n",
    "cleaned_df = mpg_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaOtVJYpcvOT",
    "outputId": "24f5665e-7a8b-4af9-e39c-54c8e790ae72"
   },
   "outputs": [],
   "source": [
    "cleaned_df[predictors].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k86l8NjNcvOT",
    "outputId": "227a5da8-5400-4bfb-8812-83f16caf8195"
   },
   "outputs": [],
   "source": [
    "cleaned_df[target].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Linear regression\n",
    "Linear regression is a fundamental statistical and machine learning method used to model and analyze the relationships between a dependent variable and one or more independent variables. The main goal of linear regression is to find the best fit straight line that accurately predicts some numerical values..\n",
    "\n",
    "Simply put, when we say we're \"plotting a linear regression\", we're trying to draw a straight line that best represents the data according to the \"least squares criterion\".\n",
    "\n",
    "## Example\n",
    "Consider a scenario where we have some data on the number of hours studied and the respective grades achieved by a group of students. Let's create a simple linear regression to see if there's a relationship between these two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data: Hours studied vs. exam scores\n",
    "data = {'Hours_Studied': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'Exam_Score': [50, 55, 62, 65, 70, 75, 78, 85]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plotting the data\n",
    "plt.scatter(df['Hours_Studied'], df['Exam_Score'], color='blue', label='Data Points')\n",
    "plt.title('Hours Studied vs. Exam Score')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, you'll observe data points that seem to follow a linear trend - as the number of hours studied increases, the exam score generally seems to increase as well.\n",
    "\n",
    "## Fitting a linear regression model\n",
    "When we say \"fitting a linear regression to the data\", we're trying to find the straight line that best represents our data. Mathematically, a straight line is represented as  \n",
    "- $y=mx+c$, where $m$ is the slope and $c$ is the y-intercept, if we have only one predictor x\n",
    "- $y=m_1x_1+m_2x_2+...+m_nx_n + c$, if we have multiple predictors $x_1, ..., x_n$\n",
    "\n",
    "In the context of our example, \"fitting\" would mean finding the best line -- the best possible slope $m^*$ and intercept $c^*$ -- that describes the relationship between hours studied and exam scores.\n",
    "\n",
    "## What does *best* mean?\n",
    "\n",
    "The code below has some candidate lines. Which one do you think is the best fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data: Hours studied vs. exam scores\n",
    "data = {'Hours_Studied': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'Exam_Score': [50, 55, 62, 65, 70, 75, 78, 85]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Generating some potential fit lines\n",
    "lines = {\n",
    "    'Line 1': [48 + 5*x for x in df['Hours_Studied']],\n",
    "    'Line 2': [45 + 6*x for x in df['Hours_Studied']],\n",
    "    'Line 3': [50 + 4.5*x for x in df['Hours_Studied']],\n",
    "    'Line 4': [70 + 0.2*x for x in df['Hours_Studied']]\n",
    "}\n",
    "\n",
    "# Plotting the data and the potential fit lines\n",
    "plt.scatter(df['Hours_Studied'], df['Exam_Score'], color='blue', label='Data Points')\n",
    "\n",
    "for line, values in lines.items():\n",
    "    plt.plot(df['Hours_Studied'], values, label=line)\n",
    "\n",
    "plt.title('Potential Fit Lines: Hours Studied vs. Exam Score')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Criterion as an Objective Function\n",
    "\n",
    "The fundamental idea behind the method of least squares is quite intuitive: Find the line (or curve) that minimizes the sum of the squared differences (residuals) between the observed values and the values that our model predicts.\n",
    "\n",
    "### A. Why Squares?\n",
    "Before diving into the math, it's worth noting why we consider the squared differences:\n",
    "1. **Positivity**: Squaring ensures that negative and positive differences don't cancel each other out.\n",
    "2. **Penalty**: Larger deviations are given more weight, meaning we're penalizing large deviations more than smaller ones.\n",
    "3. **Differentiability**: In calculus, squared functions are differentiable, which is essential for optimization techniques.\n",
    "\n",
    "### B. Mathematical Representation\n",
    "\n",
    "Suppose you have a set of data points $\\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\} $. \n",
    "Now, let's assume that the relationship between $x$ and $y$ is linear and can be represented as $ y = \\beta_0 + \\beta_1 x $, where\n",
    "- $ \\beta_0 $ is the y-intercept.\n",
    "- $ \\beta_1 $ is the slope of the line.\n",
    "\n",
    "For each data point $ x_i $, your model would then predict a y-value equal to $\\hat{y_i} = \\beta_0 + \\beta_1 x_i $.\n",
    "\n",
    "The residual (difference between observed and predicted) for this data point is $ e_i = y_i - \\hat{y_i} $.\n",
    "\n",
    "The goal of the least squares method is to find values for $ \\beta_0 $ and $ \\beta_1 $ that minimize the sum of the squared residuals:\n",
    "\n",
    "$ Q(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 $\n",
    "\n",
    "#### C. Optimization (Optional)\n",
    "\n",
    "To find the values of $\\beta_0$ and $\\beta_1$ that minimize $Q$, we set its partial derivatives with respect to $\\beta_0$ and $\\beta_1$ to zero. The resulting system of equations can be solved to get:\n",
    "- $\\beta_1 = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{n(\\sum x^2) - (\\sum x)^2} $\n",
    "- $\\beta_0 = \\frac{\\sum y - \\beta_1 \\sum x}{n}$\n",
    "Where:\n",
    "- $ n $ is the number of data points in our dataset.\n",
    "- The sums $\\sum x$, $\\sum y$, $\\sum xy$, and $\\sum x^2$ are taken over all the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create and fit the model\n",
    "model = LinearRegression()\n",
    "X = df[['Hours_Studied']]  # Features\n",
    "y = df['Exam_Score']  # Target variable\n",
    "\n",
    "model.fit(X, y)\n",
    "# Predict the values using the model\n",
    "df['Predicted_Score'] = model.predict(X)\n",
    "\n",
    "# Plotting the data, potential fit lines, and best fit line\n",
    "plt.scatter(df['Hours_Studied'], df['Exam_Score'], color='blue', label='Data Points')\n",
    "\n",
    "for line, values in lines.items():\n",
    "    plt.plot(df['Hours_Studied'], values, label=line)\n",
    "\n",
    "# Plotting the best fit line\n",
    "plt.plot(df['Hours_Studied'], df['Predicted_Score'], color='red', linestyle='--', label='Best Fit Line')\n",
    "\n",
    "plt.title('Potential Fit Lines and Best Fit Line: Hours Studied vs. Exam Score')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One  advantage of fitting a linear regression model is that it allows us to inspect easily the coefficients of the predictor variables. In our case, we can see how much the exam score is expected to increase (or decrease) for every additional hour of study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the coeffs\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that each additional hour of study is associated with a 4.8 point increase in the exam score.\n",
    "- Note: this is just a suggestion, and not necessarily a causal relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuwdNeLNcvOT"
   },
   "source": [
    "# 1.4 Back to cars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c62KEZBacvOU",
    "outputId": "109555e7-efd0-4ecd-f204-68c35e7769b9"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# build a model\n",
    "linear = linear_model.LinearRegression()\n",
    "\n",
    "# fit the model to the data!\n",
    "# the first argument is the predictors, the second argument the target variable!\n",
    "linear.fit(cleaned_df[predictors], cleaned_df[target])\n",
    "\n",
    "# inspect the coefficients (zip joins two tuples together)\n",
    "pd.DataFrame([dict(zip(predictors, linear.coef_))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyHkgZF-cvOU"
   },
   "source": [
    "We fitted our linear regression. Let's now see how it predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEqzsfulcvOU",
    "outputId": "ddccd585-1777-4fa8-8d3a-b1b4d6d2d2c3"
   },
   "outputs": [],
   "source": [
    "# get some predictions from the model\n",
    "preds = linear.predict(cleaned_df[predictors])\n",
    "\n",
    "predictions_df = cleaned_df.assign(predictions=preds)\n",
    "\n",
    "predictions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjM0Otg-cvOU",
    "outputId": "badd85d4-0ea1-4c66-a652-c57ee8850e47"
   },
   "outputs": [],
   "source": [
    "# and lets try a scatter plot of our predicted mpg and the true value\n",
    "predictions_df.plot(kind=\"scatter\", \n",
    "                    x=\"mpg\", \n",
    "                    y=\"predictions\", \n",
    "                    c='forestgreen', \n",
    "                    xlim=(0,50), \n",
    "                    ylim=(0,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why didn't we plot the best fitting line -- like before?\n",
    "The reason is that it is easier to plot the best fitting line on paper when we have only one predictor. However, when we have multiple predictors, it is impossible to plot the best fitting line on paper. Instead, we can plot the actual values of the target variable against the predicted values of the target variable."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "06.2 Learning a linear model from data (linear regression).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
