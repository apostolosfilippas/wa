{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🎓 **Professor**: Apostolos Filippas\n",
    "\n",
    "### 📘 **Class**: Web Analytics\n",
    "\n",
    "### 📋 **Topic**: Evaluating Models\n",
    "\n",
    "### 🔗 **Link**: https://bit.ly/WA_LEC8_EVAL\n",
    "\n",
    "### 🛢️ **Data**: https://bit.ly/mailingData \n",
    "\n",
    "🚫 **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Starting out\n",
    "\n",
    "We start off as we usually do. Let's import some things that will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas to read in data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import matplotlib for plotting\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import decision trees and logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import train, test, and evaluation functions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data\n",
    "We're going to use a mail response data set from a real direct marketing campaign located in `files/mailing.csv`. \n",
    "\n",
    "You can download the files from [https://bit.ly/mailingData](https://bit.ly/mailingData).\n",
    "\n",
    "\n",
    "Each record represents an individual who was targeted with a direct marketing offer.  The offer was a solicitation to make a charitable donation. \n",
    "\n",
    "The columns (features) are:\n",
    "\n",
    "```\n",
    "Col.  Name      Description\n",
    "----- --------- ----------------------------------------------------------------\n",
    "1     income    household income\n",
    "2     Firstdate data assoc. with the first gift by this individual\n",
    "3     Lastdate  data associated with the most recent gift \n",
    "4     Amount    average amount by this individual over all periods (incl. zeros)\n",
    "5     rfaf2     frequency code\n",
    "6     rfaa2     donation amount code\n",
    "7     pepstrfl  flag indicating a star donator\n",
    "8     glast     amount of last gift\n",
    "9     gavr      amount of average gift\n",
    "10    class     one if they gave in this campaign and zero otherwise.\n",
    "```\n",
    "\n",
    "Our goal is to build a model to predict if people will give during the current campaign (this is the attribute called `\"class\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read our data in and put the target variable in `Y` and all the other features in `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data using pandas\n",
    "data = pd.read_csv(\"files/mailing.csv\")\n",
    "\n",
    "# Split into X and Y\n",
    "X = data.drop(columns=['class'])\n",
    "Y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Overfitting\n",
    "\n",
    "Let's first create a classification algorithm called \"decision tree,\" fit a model (learn the model from the data), and use it to get predictions on all of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty, unlearned tree\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "# Fit/train the tree\n",
    "tree.fit(X, Y)\n",
    "\n",
    "# Get a prediction\n",
    "Y_predicted = tree.predict(X)\n",
    "\n",
    "# Get the accuracy of this prediction\n",
    "accuracy = accuracy_score(Y_predicted, Y)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"The accuracy is {100*accuracy:2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty high accuracy. Is it? Let's check the base rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% of the people do not donate --> 99.5% accuracy is pretty good (much higher than the base rate of 95%).\n",
    "\n",
    "However, we might be overfitting our data. The model might have \"memorized\" where all the points are. This does not lead to models that will generalize well.\n",
    "\n",
    "We can create training and testing sets very easily. Here we will create train and test sets of `X` and `Y` where we assign 70% of our data to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and Y into training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the same decision tree but fit it with our training data and test it on our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty, unlearned tree\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "# Fit/train the tree on the training data\n",
    "tree.fit(X_train, Y_train)\n",
    "\n",
    "# Get a prediction from the tree on the test data\n",
    "Y_test_predicted = tree.predict(X_test)\n",
    "\n",
    "# Get the accuracy of this prediction\n",
    "accuracy = accuracy_score(Y_test_predicted, Y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"The accuracy is {100*accuracy:2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use cross validation with 5 folds to see how well our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty, unlearned tree\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    \n",
    "# This will get us 5-fold cross validation accuracy with our tree and our data\n",
    "# We can do this in one line!\n",
    "cross_fold_accuracies = cross_val_score(tree, X, Y, scoring=\"accuracy\", cv=5)\n",
    "    \n",
    "# Average accuracy\n",
    "average_cross_fold_accuracy = np.mean(cross_fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in cross_fold_accuracies:\n",
    "    print(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_cross_fold_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty big difference! Which accuracy do you \"trust\" more? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating a simple learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(9001)\n",
    "\n",
    "# do an 80/20 split of the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.70)\n",
    "\n",
    "# Here are some percentages to get you started. Feel free to try more!\n",
    "training_percentages = [i*0.1 -0.0001 for i in range(1, 11)]\n",
    "accuracies = []\n",
    "\n",
    "for training_percentage in training_percentages:\n",
    "    X_temp_train, X_temp_test, Y_temp_train, Y_temp_test = train_test_split(X_train, Y_train, train_size=training_percentage)\n",
    "\n",
    "    # This will create an empty logistic regression\n",
    "    #logistic = LogisticRegression()\n",
    "    tree = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "    \n",
    "    # This will fit/train your logistic regression\n",
    "    #logistic.fit(X_train, Y_train)\n",
    "    tree.fit(X_temp_train, Y_temp_train)\n",
    "\n",
    "    \n",
    "    # This will get predictions\n",
    "    #Y_test_predicted = logistic.predict(X_test)\n",
    "    Y_test_predicted = tree.predict(X_test)\n",
    "\n",
    "    \n",
    "    # With these predictions we can get an accuracy. Where should we store this accuracy?\n",
    "    acc = accuracy_score(Y_test_predicted, Y_test)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "# We want to plot our results. What list should we use for the x-axis? What about the y-axis?\n",
    "plt.plot(training_percentages, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creating a simple fitting curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit our training data size to 80%\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80)\n",
    "\n",
    "# Let's try different max depths for a decision tree\n",
    "max_depths = range(1, 100)\n",
    "accuracies = []\n",
    "accuracies_train = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    # This will create an empty decision tree at a specified max depth\n",
    "    tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth)\n",
    "    \n",
    "    # This will fit/train your tree\n",
    "    tree.fit(X_train, Y_train)\n",
    "    \n",
    "    # This will get accuracy and keep track of it\n",
    "    Y_test_predicted  = tree.predict(X_test)\n",
    "    Y_train_predicted = tree.predict(X_train)\n",
    "    accuracies.append(accuracy_score(Y_test_predicted, Y_test))\n",
    "    accuracies_train.append(accuracy_score(Y_train_predicted, Y_train))\n",
    "\n",
    "# We want to plot our results\n",
    "plt.plot(max_depths, accuracies)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Max depth (model complexity)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Max depth (model complexity)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: The ROC curve\n",
    "\n",
    " The ROC (Receiver Operating Characteristic) curve is a staple in predictive modeling, especially for binary classification problems. It provides us with a graphical representation of a model's true positive rate vs. its false positive rate, over various threshold values.\n",
    "\n",
    " ## A short logistic regression introduction\n",
    "Logistic regression is another statistical method for analyzing datasets.\n",
    "- In linear regression, we  were trying to fit a line\n",
    "$$y = b_0 + b_1 x_1 + \\ldots + b_n x_n$$\n",
    "- In logistic function, we are trying to fit a **sigmoid function**\n",
    "$$y = 1 / (1 + e^{-(b_0 + b_1 x_1 + \\ldots + b_n x_n)})$$\n",
    "- Similar to linear regression, our goal during fitting is to estimate the \"best\"  parameters $b_0, b_1, \\ldots, b_n$.\n",
    "\n",
    " Note that the prediction of the logistic regression will always be avalue between 0 and 1. This allows us to interpret the prediction as the probability that an instance belongs to a target class.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a simple sigmoid function\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ploit several sigmoid functions with different slopes and intercepts\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y1 = 1 / (1 + np.exp(-x))\n",
    "y2 = 1 / (1 + np.exp(-2*x))\n",
    "y3 = 1 / (1 + np.exp(-0.5*x))\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y2)\n",
    "plt.plot(x, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a logistic regression model\n",
    "\n",
    " Let's first create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample Dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert training data to DataFrame\n",
    "df_train = pd.DataFrame(X_train, columns=[f'Feature_{i+1}' for i in range(X_train.shape[1])])\n",
    "df_train['True_Label'] = y_train\n",
    "\n",
    "# Convert test data to DataFrame\n",
    "df_test = pd.DataFrame(X_test, columns=[f'Feature_{i+1}' for i in range(X_test.shape[1])])\n",
    "df_test['True_Label'] = y_test\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the model to the training data and get the predicted probabilities for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "clf = LogisticRegression()\n",
    "# fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "# get predictions on train and test data\n",
    "train_probabilities = clf.predict_proba(X_train)[:,1]\n",
    "test_probabilities = clf.predict_proba(X_test)[:,1]\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"best\" coefficients that we learnt are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best coefficients we've learnt\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the predicted labels, if we use a threshold of 0.5 (the default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Predicted_Label'] = train_predictions\n",
    "df_train['Predicted_Probability'] = train_probabilities\n",
    "df_test['Predicted_Label'] = test_predictions\n",
    "df_test['Predicted_Probability'] = test_probabilities\n",
    "\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the base rate of the dataset\n",
    "print(f'Base Rate: {np.round(1 - df_test[\"True_Label\"].mean(), 4)}')\n",
    "\n",
    "\n",
    "# get accuracy on train and test data\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "print(f'Train Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the optimal threshold\n",
    "\n",
    "Even though we used 0.5 as the threshold, we can use any threshold we want. In fact, it might be better to use a different threshold based on the problem we are trying to solve, i.e how much we care about true positives vs. false positives vs. true negatives vs. false negatives.\n",
    "\n",
    "The ROC curve helps us visualize what would have happened had we picked different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_probabilities)\n",
    "\n",
    "# Selected thresholds\n",
    "selected_thresholds = [1, 0.75, 0.5, 0.25, 0]\n",
    "selected_indices = [np.argmin(np.abs(thresholds-t)) for t in selected_thresholds]\n",
    "\n",
    "# Visualization adjustments\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "\n",
    "# Scatter plot for selected thresholds\n",
    "plt.scatter(fpr[selected_indices], tpr[selected_indices], color='black', s=50, label='Selected thresholds')\n",
    "for ind in selected_indices:\n",
    "    plt.annotate(f\"{thresholds[ind]:.2f}\", (fpr[ind], tpr[ind]), textcoords=\"offset points\", xytext=(-10,-10), ha='center')\n",
    "\n",
    "# Adjust axis limits for better visibility\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve with Selected Thresholds')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 45 degree line represents a random guess of probability, and then classifying based on this random guess and the corresponding threshold. I.e., for a threshold = 1, we would classify everything as negative; for a threshold = 0, we would classify everything as positive; and so on.\n",
    "- The curve represents the performance of our model at different thresholds. The higher the curve, the better the model.\n",
    "- The area under the curve (AUC) is a measure of how good the model is. The higher the AUC, the better the model.\n",
    "  \n",
    "More https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
