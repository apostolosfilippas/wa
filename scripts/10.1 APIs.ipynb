{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üéì **Professor**: Apostolos Filippas (with Reanna Ishmael's help)\n",
    "\n",
    "### üìò **Class**: Web Analytics\n",
    "\n",
    "### üìã **Topic**: APIs\n",
    "\n",
    "### üîó **Link**: [https://bit.ly/WA_LEC10_API](https://bit.ly/WA_LEC10_API)\n",
    "\n",
    "### üõ¢Ô∏è **Data**: [http://bit.ly/WA_LEC10_DATA](https://bit.ly/WA_LEC10_DATA)\n",
    "\n",
    "üö´ **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö™ 1. Introduction\n",
    "\n",
    "## What's an API?\n",
    "\n",
    "An API, or Application Programming Interface, is a set of protocols and tools that allow different software applications to communicate with each other. \n",
    "- **It can be thought of as a contract**, where one software promises to provide certain functionalities (through endpoints) in response to a specific set of input parameters.\n",
    "- Think of an API as a menu in a restaurant. The menu provides a list of dishes you can order, along with a description of each dish. When you specify what menu items you want, the restaurant's kitchen does the work and provides you with some finished dishes. You don't know exactly how the restaurant prepares that food, and you don't really need to. \n",
    "- Similarly, an API lists a bunch of operations that developers can use, along with a description of what they do. The developer doesn't necessarily need to know how, for example, an operating system builds and manages a file system; they just need to know that there's an API call that can be used to list files in a directory.\n",
    "\n",
    "Pretty much every single command you use can be thought of as an API.\n",
    "\n",
    "## Why are APIs important?\n",
    "APIs play a pivotal role in today's digital world. \n",
    "- They allow for the integration of systems, making it possible for applications to share data and functionalities seamlessly. \n",
    "- They allow for scalability by decoupling server-side functionality from the client side\n",
    "- They allow easy access to third-party tools and functionalities\n",
    "\n",
    "\n",
    "## Accessing APIs\n",
    "APIs can be \n",
    "1. **Open/Public**: No restrictions to access these types of APIs because they are publicly available. \n",
    "2. **Partner**: These APIs are typically offered to a select number of partners, usually through an agreement. They may require some form of authentication or access key.\n",
    "3. **Internal**: These APIs are usually not exposed to the public. They are used to build internal systems and functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Architecture\n",
    "APIs can be classified based on their architecture. The most common types of APIs are:\n",
    "\n",
    "1. **REST**: Representational State Transfer. REST APIs are the most common type of APIs. They are based on the HTTP protocol and are usually stateless. They are also known as RESTful APIs.\n",
    "2. **GraphQL**: It is a query language for APIs. It is not a standard, meaning that it is not governed by a set of rules and specifications. It is also stateless, meaning that the server does not store information about the client session. It is also cacheable, meaning that the client can store a copy of the response for future use. It is also uniform, meaning that the same API call will always return the same response. Finally, it is layered, meaning that the client does not need to know the internal workings of the server in order to use the API.\n",
    "\n",
    "3. **gRPC**: It is a remote procedure call framework developed by Google. It is based on the HTTP/2 protocol and uses the Protocol Buffers data serialization format. It is a standard, meaning that it is governed by a set of rules and specifications. It is also stateless, meaning that the server does not store information about the client session. It is also cacheable, meaning that the client can store a copy of the response for future use. It is also uniform, meaning that the same API call will always return the same response. Finally, it is layered, meaning that the client does not need to know the internal workings of the server in order to use the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Making API Calls\n",
    "\n",
    "The most essential library for API calls is your old favorrite `requests` library. As we learned, it abstracts many of the complexities of making requests, making it straightforward to send HTTP requests and handle API responses.\n",
    "\n",
    "## Example 1 - dog facts\n",
    "Here's an example of an API call of a public REST API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some \"dog facts\" hitting a simple API with a GET request\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://dogapi.dog/api/v2/facts?limit=2\") # Print only the first two facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the status code of the response.\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the request was successful\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Print the response body as text\n",
    "facts = response.json().get(\"data\", [])  # Access the \"data\" key which contains the facts\n",
    "for fact in facts:  \n",
    "    for key, value in fact.items():\n",
    "        print(f'{key}:\\t{value}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation here:\n",
    "- https://dogapi.dog/docs/api-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - Star Wars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://swapi.dev/api/people/1\")\n",
    "response.raise_for_status()\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://swapi.dev/api/people/2\")\n",
    "response.raise_for_status()\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://swapi.dev/api/planets/1\")\n",
    "response.raise_for_status()\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://swapi.dev/api/starships/2\")\n",
    "response.raise_for_status()\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. The OpenAI Chat Completions Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Overhead\n",
    "\n",
    "First, make sure you have installed the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai\n",
    "# pip install tiktoken\n",
    "# pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up imports, API key, and client for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the environment variables in the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the environment variable with the name OPENAI_API_KEY\n",
    "my_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not my_api_key:\n",
    "  print(\"WARNING: OpenAI API key not found!!!\")\n",
    "else:\n",
    "  print(\"Successfully retrieved OpenAI API key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI client\n",
    "# This will allow you to use their API without having to manually make HTTP requests\n",
    "client = OpenAI(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.2 Making a request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the before-class notebook, we made our first request to the OpenAI API. Let's break that process down.\n",
    "- We're making a request to the chat completions endpoint, as indicated by `chat.completions`. \n",
    "- This endpoint is a similar to ChatGPT \n",
    "- It takes a message (or list of messages) provided by the user and it returns a message generated by the model as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"Tell me a fun fact about cats.\",\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"Tell me a fun fact about professors.\",\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"Write some Python code.\",\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the request above, we included two parameters: `model` and `messages`.\n",
    "\n",
    "- `model`: A string with the name of the model we want to use. A full list of models and the endpoints they are compatible with is available [here](https://platform.openai.com/docs/models/overview).\n",
    "\n",
    "- `messages`: A list of dictionaries. Each dictionary has two keys: `role` and `content`. \n",
    "  - The value of `content` is a message in the conversation between the user and the model. \n",
    "  - The value of `role` refers to the author of that message, and can have one three specific values:\n",
    "    - `system`: Think of this as what is prepended to the whole text that we feed in the LLM. Its role is to modify the behavior of the LLM.\n",
    "    - `user`: This is our message -- what we are sending to the model to complete.\n",
    "    - `assistant`: This refers to the model itself. It is the response that the model generates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how we can use the `messages` parameter to simulate a conversation between the user and the model, giving the model an example of the type of output we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    { \n",
    "      \"role\": \"system\", \n",
    "      \"content\": \"You are a fantastic film critic. You rate movies using simple terms, only outputing the word 'negative' or 'positive'.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"Wonderful spectacle, terrific acting and toweringly great film-making.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\", \n",
    "      \"content\": \"positive\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"A disappointing entry in the Scorsese canon.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\", \n",
    "      \"content\": \"negative\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"Gangs Of New York is a magnificent film, and is challengingly about more than the sum of its parts.\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` and `messages` parameters are required for any request to the chat completion output. However, we can provide other optional parameters that influence our output, such as:\n",
    "\n",
    "- `temperature`: A decimal value between 0 and 2. Higher values of temperature produce more creative outputs, while lower values lead to more focused and deterministic responses.\n",
    "- `max_tokens`: The maximum number of tokens for the model to generate. If left unspecified, the model will keep going until it has finished its answer (think ChatGPT), or until the response has exceeded the model's context length.\n",
    "- `n`: The number of chat completion outputs to generate for each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"Tell me a fun fact about cats.\",\n",
    "    }\n",
    "  ],\n",
    "  temperature=2,\n",
    "  max_tokens=200\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.4 The response object\n",
    "\n",
    "So far, we've been using the following command to print the content of the model's response directly:\n",
    "\n",
    "```\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "Now, we're going to take a closer look at what the OpenAI API returns to us. \n",
    "\n",
    "Let's make another request. This is the same as the earlier request for cat facts, except we've now set `n=2` to get two outputs instead of just one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    { \n",
    "      \"role\": \"system\", \n",
    "      \"content\": \"You are a comedian with dark, biting humor.\",\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"Tell me a joke about professors.\"\n",
    "    }\n",
    "  ],\n",
    "  n=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API returns a `ChatCompletion` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, the `ChatCompletion` object is somewhat like a nested dictionary. \n",
    "\n",
    "Instead of using bracket notation to navigate (e.g., `response[\"choices\"]`), we use dot notation, like in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip:** For the purpose of parsing the OpenAI response objects, you can assume that anything beginning with an equal sign in the print output will require you to use dot notation to access it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing `response.choices` gives us a list of the outputs generated by the model.\n",
    "\n",
    "For the requests we made in previous sections, we didn't change the parameter `n`. As a result, we had only one item in the list by default, and we accessed that item using the index 0.\n",
    "\n",
    "In the request we just made, we set `n` equal to 2. This gives us two cat facts, which we can access as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  print(\"Message content:\", response.choices[i].message.content)\n",
    "  print(\"Role:\", response.choices[i].message.role)\n",
    "  print(\"Finish reason:\", response.choices[i].finish_reason)\n",
    "  print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are generally interested in the message content. However, as we see above, the API also gives us some metadata. For instance, `finish_reason` tells us why the model stopped generating output (e.g. it hit a natural stopping point, it ran out of tokens, etc.).\n",
    "\n",
    "In the next section, we'll look at one aspect of the metadata that is of practical interest: tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.5 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 What is a token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you make a request to the OpenAI API, your natural language inputs are converted into tokens. Similarly, the model also returns tokens in its output. \n",
    "\n",
    "A token is a common sequence of characters. Usually, a token is either a word or a word fragment, but it can also contain digits and symbols.\n",
    "\n",
    "We can use the [online OpenAI tokenizer](https://platform.openai.com/tokenizer) to see how text maps to tokens. Here's an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"https://drive.google.com/uc?id=1zn4q6_RyIUpKRY8MBNvnE6Agw_LnlTEL\" \n",
    "    width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we observe that:\n",
    "\n",
    "- Some words map directly to tokens, like \"how\" and \"use\"\n",
    "- Other words, like \"we're\" and \"OpenAI,\" are broken up into multiple tokens\n",
    "- The period at the end of the sentence is also a token by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Tokenizing in Python\n",
    "\n",
    "It's useful to compute the number of tokens directly in Python. We can do this using the `tiktoken` package created by OpenAI.\n",
    "\n",
    "Calling `encoding.encode` will give us a list of ids. Each id corresponds to a token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "sentence = \"We're learning how to use the OpenAI API in class today.\"\n",
    "\n",
    "# Retrieve the encoding for our model\n",
    "encoding = tiktoken.encoding_for_model(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "sentence_encoding = encoding.encode(sentence)\n",
    "\n",
    "print(sentence_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the number of tokens by getting the length of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert our list of ids back into natural language by calling `encoding.decode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.decode(sentence_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Why do we care?\n",
    "\n",
    "We count tokens for a few practical reasons:\n",
    "\n",
    "- **Billing:** For the chat completions endpoint, you are charged by the number of tokens in your input and output. If you're making a huge number of API requests, you might want to reduce the size of your prompt to reduce costs. \n",
    "- **Model constraints:** Models have a context window, or a maximum number of tokens that they can process. In other words, you can't give the model a whole novel at once. For gpt-3.5-turbo, this context window is 4,096 tokens. Requests with more tokens than that will throw an error.\n",
    "- **Time:** The more tokens in the model's response, the longer the response generation time. Depending on what you're trying to achieve, it might be desirable to limit the number of tokens in the model's response with `max_tokens`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the token usage from our last request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens from our input messages (the prompt\"\n",
    "print(\"Prompt tokens:\", response.usage.prompt_tokens)\n",
    "\n",
    "# Tokens from the model's response\n",
    "print(\"Completion tokens:\", response.usage.completion_tokens) \n",
    "\n",
    "print(\"Total tokens:\", response.usage.total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of November 11, 2023, the [OpenAI pricing page](https://openai.com/pricing) states the price for gpt-3.5-turbo calls as follows:\n",
    "- $0.0010 / 1k tokens for the input\n",
    "- $0.0020 / 1k tokens for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cost = (response.usage.prompt_tokens / 1000) * 0.0010\n",
    "output_cost = (response.usage.completion_tokens / 1000) * 0.0020\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(f\"The total cost of our last request was ${total_cost:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, you can make a lot of requests without really worrying about cost. So feel free to play around with the API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# 4. The OpenAI Embeddings endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Vector representations of text\n",
    "\n",
    "Say that we're trying to build a search engine and we want to return the document most relevant to the user's search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"deep learning models\"\n",
    "\n",
    "documents = [\n",
    "    \"deep cleaning services\",\n",
    "    \"chicago deep dish pizza\",\n",
    "    \"artificial neural networks\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by turning our text into numeric data using the CountVectorizer, which we covered in our last class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add query to the list of documents\n",
    "documents.append(query)\n",
    "\n",
    "# Vectorizer from last class\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Create a DataFrame to show the vector representations of each sentence\n",
    "vector_df = pd.DataFrame(matrix.toarray(), \n",
    "                         index=documents, \n",
    "                         columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "vector_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the representation above, we have a **vector** for each mini document. \n",
    "- Each value in the vector corresponds to the count of a particular word within that document.\n",
    "- you can think of the values in the vector as coordinates in a high-dimensional space.\n",
    "\n",
    "Now, let's take it a step further and compute the similarities between each document and the user's query. To do this, we'll use the **cosine similarity function** from scikit-learn.\n",
    "- Cosine similarity is a metric that lies on the interval [-1, 1] (or [0, 1], if the elements of the vector are non-negative). \n",
    "- The geometrical interpretation of cosine similarity is that it measures the cosine of the angle between two vectors.\n",
    "- The higher the value of cosine similarity, the more similar the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the similarities between the texts\n",
    "cosine_similarities = cosine_similarity(matrix)\n",
    "\n",
    "# Create a DataFrame with those similarities\n",
    "cosine_similarity_df = pd.DataFrame(cosine_similarities, index=documents, columns=documents)\n",
    "\n",
    "# Find the documents most similar to the query (other than the query itself)\n",
    "cosine_similarity_df = cosine_similarity_df.loc[query].drop(query)\n",
    "\n",
    "# Rank in order of similarity score, from highest to lowest\n",
    "cosine_similarity_df = cosine_similarity_df.sort_values(ascending=False)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"----------------------------------------\")\n",
    "print(cosine_similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we notice from inspecting these values?\n",
    "\n",
    "- Even though \"artificial neural networks\" is conceptually closer to the user's query than \"deep cleaning services\" or \"chicago deep dish pizza,\" it's ranked last in terms of similarity because there aren't any words in common.\n",
    "\n",
    "This example presents one issue with word count representations, like the ones generated by CountVectorizer: they neglect the **meaning** of each document. \n",
    "\n",
    "As a result, it's harder for us to perform tasks that rely on meaning, like clustering documents related to similar topics (e.g., in order to build a recommender system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 OpenAI's embeddings\n",
    "\n",
    "OpenAI's embeddings take advantage of the vast array of data that the models were trained on. The model convert text to a vector representation that more accurately captures the semantic information of the text.\n",
    "\n",
    "Let's make API requests to get the embeddings for each of our sample documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, document):\n",
    "  response = client.embeddings.create(\n",
    "    model=model,\n",
    "    input=document,\n",
    "    encoding_format=\"float\"\n",
    "  )\n",
    "  return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary that maps each document to the corresponding embedding from OpenAI:\n",
    "\n",
    "> Note: This dict comprehension is mainly for example purposes. If you are performing a large number of requests, please save the embeddings to disk periodically, because if your code halts (for whatever reason), you don't want to lose that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {doc: get_embedding(model=\"text-embedding-ada-002\", document=doc) for doc in documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting our results shows us that the text-embedding-ada-002 model returns an embedding vector with 1,536 elements for every request. Each element of our embedding vector is a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, embedding in embeddings.items():\n",
    "  print(\"Document:\", doc)\n",
    "  print(\"Length of embedding vector:\", len(embedding))\n",
    "  print(\"First five elements:\", embedding[:5])\n",
    "  print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the cosine similarities between documents again, using the OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the embeddings from OpenAI into a matrix\n",
    "embeddings_matrix = np.array([embedding for embedding in embeddings.values()])\n",
    "\n",
    "# Use the matrix to calculate the similarities between the texts\n",
    "cosine_similarities = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# Create a DataFrame with those similarities\n",
    "cosine_similarity_df = pd.DataFrame(cosine_similarities, index=documents, columns=documents)\n",
    "\n",
    "# Find the documents most similar to the query (other than the query itself)\n",
    "cosine_similarity_df = cosine_similarity_df.loc[query].drop(query)\n",
    "\n",
    "# Rank in order of similarity score, from highest to lowest\n",
    "cosine_similarity_df = cosine_similarity_df.sort_values(ascending=False)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"----------------------------------------\")\n",
    "print(cosine_similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the similarity numbers are higher across the board, we see that \"artificial neural networks\" now outranks the other documents. This updated ranking reflects the semantic information that was not captured by word counts alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Overhead\n",
    "\n",
    "Now that we have gained some familiarity with the OpenAI API, we'll learn how to use the `llama-index` package.\n",
    "- `llama-index` relies on the power of LLMs to perform RAG (retrieval-augmented generation) tasks\n",
    "- Plainly, this means imbuing LLMs with knowledge from our own data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.core import (\n",
    "  SimpleDirectoryReader,\n",
    "  StorageContext,\n",
    "  load_index_from_storage,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Question answering\n",
    "\n",
    "**How does Q&A with `llama-index` work?**\n",
    "\n",
    "**Step 1: Loading**\n",
    "\n",
    "First, we will load our document from file using the `SimpleDirectoryReader`.\n",
    "\n",
    "To answer questions about our document, `llama-index` needs to convert the document into a format that allows for semantic search. This involves breaking the document up into smaller sections called nodes.\n",
    "\n",
    "**Step 2: Indexing**\n",
    "\n",
    "Then, we create a `VectorStoreIndex`. \n",
    "\n",
    "Then, `llama-index` calls the OpenAI embeddings API to convert the text of these nodes into contextual embeddings (vectors with floats), just like we saw in the previous section. By default, it uses the text-embedding-ada-002 model.\n",
    "\n",
    "**Step 3: Persisting**\n",
    "\n",
    "Calling the embeddings API costs money and relies on your OpenAI API key. Therefore, we call `index.storage_context.persist`, which stores our index to disk so that we don't have to reindex our document more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll start by loading and indexing a simple job posting. We'll also save this index to disk, so we don't need to re-index it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the file here\n",
    "data_filepath = \"PATH/TO/YOUR/DIRECTORY\"\n",
    "# llama index will create this folder\n",
    "storage_dir = \"storage\"\n",
    "\n",
    "# Check if storage already exists\n",
    "if not os.path.exists(storage_dir):\n",
    "  \n",
    "  # Load the documents and create the index\n",
    "  documents = SimpleDirectoryReader(data_filepath).load_data()\n",
    "  index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "  # Store it for later\n",
    "  index.storage_context.persist()\n",
    "\n",
    "else:\n",
    "    \n",
    "  storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "  index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Create a query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Querying**\n",
    "\n",
    "Now comes the fun part: querying! Let's ask some questions about our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What programming languages do I need to know for this job?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What degree is required for this job?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How many weeks will I be working for?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Extracting structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-index-program-evaporate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.program.openai import OpenAIPydanticProgram\n",
    "from llama_index.program.evaporate.df import DFRowsProgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a large block of text with several job listings. Let's use `llama-index` to extract relevant data in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Adobe is seeking talented and passionate Software Engineer interns across all organizations.\n",
    "All 2024 Adobe interns will be co-located hybrid.\n",
    "You need proficiency and experience with the following: Java, C++, JavaScript, Python.\n",
    "The U.S. pay range for this position is $45.00 -- $55.00 hourly.\n",
    "\n",
    "At Amazon, we hire the best minds in technology to innovate and build on behalf of our customers. \n",
    "Programming experience with at least one modern language such as Java, C++, or C# including object-oriented design is required.\n",
    "The base pay for a Software Development Engineer Intern ranges from $42.50/hr in our lowest geographic market up to $96.15/hr in our highest geographic market.\n",
    "The majority of our SDE roles are based in the greater Seattle/Bellevue, WA area.\n",
    "\n",
    "\n",
    "Zoox is looking for a system engineering intern to join our Systems Design and Mission Assurance (SDMA) team. \n",
    "You need experience working with vehicle dynamics simulation tools such as Carmaker and/or Carsim, as well as experience with Python.\n",
    "You will work with a cross functional team in Foster City, CA to evaluate the autonomous vehicle‚Äôs response to various electrical/mechanical faults in the motion control system.\n",
    "We are data-driven, transparent, and consistent. The target rate for this role is $50-$74.51/hr.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an empty dataframe containing the fields you want to extract from the text, along with their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Employer\": pd.Series(dtype=\"str\"),\n",
    "        \"Position\": pd.Series(dtype=\"str\"),\n",
    "        \"Python Required\": pd.Series(dtype=\"bool\"),\n",
    "        \"C++ Required\": pd.Series(dtype=\"bool\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a program that will extract rows from the text, using your existing dataframe schema\n",
    "df_rows_program = DFRowsProgram.from_defaults(\n",
    "    pydantic_program_cls=OpenAIPydanticProgram, df=df\n",
    ")\n",
    "\n",
    "# Use the program to parse the text and generate rows\n",
    "result_obj = df_rows_program(input_str=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we turn the results into a dataframe so we can explore the data and perform analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from our result object\n",
    "dataframe_rows = []\n",
    "for row in result_obj.rows:\n",
    "  dataframe_rows.append(row.row_values)\n",
    "  \n",
    "jobs = pd.DataFrame(dataframe_rows, columns=[\"Employer\", \"Position\", \"Python Required\", \"C++ Required\"])\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Yes\"/\"No\" to True/False if column names are correct\n",
    "jobs[\"Python Required\"] = jobs[\"Python Required\"].map({\"Yes\": True, \"No\": False})\n",
    "jobs[\"C++ Required\"] = jobs[\"C++ Required\"].map({\"Yes\": True, \"No\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs[jobs[\"Python Required\"] & ~jobs[\"C++ Required\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_filtered = jobs[(jobs[\"Python Required\"] == True) & (jobs[\"C++ Required\"] != True)]\n",
    "# jobs_filtered = jobs[(jobs[\"Python Required\"] == \"Yes\") & (jobs[\"C++ Required\"] != \"Yes\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs[jobs[\"Python Required\"] & ~jobs[\"C++ Required\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Unsupervised data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.program.evaporate.df import DFFullProgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do something pretty awesome with `llama-index`. If you don't want to specify the dataframe schema manually, you can leave it up to the LLM to figure out what to extract by running `DFFullProgram` instead of `DfRowsProgram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_program = DFFullProgram.from_defaults(\n",
    "    pydantic_program_cls=OpenAIPydanticProgram,\n",
    ")\n",
    "\n",
    "result_obj = df_full_program(input_str=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = result_obj.to_df()\n",
    "\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataframe schema is generated by the LLM, it might take a little more work to get the data into a usable format, but this is still a lot more efficient than going through the data yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_rate_cols = [\"Min Hourly Rate\", \"Max Hourly Rate\"]\n",
    "\n",
    "jobs[hourly_rate_cols] = jobs[\"Pay Range\"].str.extract(r\"\\$(\\d+\\.\\d+) - \\$(\\d+\\.\\d+)\")\n",
    "\n",
    "jobs[hourly_rate_cols] = jobs[hourly_rate_cols].astype(float)\n",
    "\n",
    "jobs.sort_values(by=\"Max Hourly Rate\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Querying pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-index llama-index-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.experimental.query_engine import PandasQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = PandasQueryEngine(df=jobs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `llama-index` can also convert your natural language queries directly into pandas commands and run those commands on your dataframe. This might come in handy if you were trying to build an application like a data chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is the max hourly rate of jobs that require Python?\")\n",
    "\n",
    "print(f\"The max hourly rate is: ${response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
