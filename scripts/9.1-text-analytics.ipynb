{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nKtLvTqR-or"
   },
   "source": [
    "---\n",
    "\n",
    "### üéì **Professor**: Apostolos Filippas\n",
    "\n",
    "### üìò **Class**: Web Analytics\n",
    "\n",
    "### üìã **Topic**: Pandas (self-study)\n",
    "\n",
    "### üîó **Link**: https://bit.ly/WA_LEC9_TEXT\n",
    "\n",
    "### üõ¢Ô∏è **Data**: http://bit.ly/someBookReviews \n",
    "\n",
    "üö´ **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö™ 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_GoKxQRR-ot"
   },
   "source": [
    "As always, we'll import the packages we'll need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1127,
     "status": "error",
     "timestamp": 1554408488826,
     "user": {
      "displayName": "Apostolos Filippas",
      "photoUrl": "https://lh4.googleusercontent.com/-APa8NqBX0rQ/AAAAAAAAAAI/AAAAAAAAEFE/Lg41AVGHJ0c/s64/photo.jpg",
      "userId": "01329202069636780243"
     },
     "user_tz": 240
    },
    "id": "hrFZ3wBAR-ot",
    "outputId": "e2fbb5fa-332a-43c1-8b49-afac2121a52f"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ig4B5tqNR-ox"
   },
   "source": [
    "## Data\n",
    "The file that I've placed in `./files/books.csv` contains 2,000 Amazon book reviews. \n",
    "\n",
    "The data set contains two features\n",
    "- the first column (contained in quotes) is the review text. \n",
    "- the second column is a binary label indicating if the review is positive or negative.\n",
    "\n",
    "Let's read the data into a pandas data frame. \n",
    "- You'll notice two new attributes in `pd.read_csv()` that we've never seen before. The first, `quotechar` is tell us what is being used to \"encapsulate\" the text fields. Since our review text is surrounding by double quotes, we let pandas know. We use a `\\` since the quote is also used to surround the quote. This backslash is known as an escape character. We also let pandas now this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwGShs6OR-o0"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"files/books.csv\", quotechar=\"\\\"\", escapechar=\"\\\\\")\n",
    "\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[50]['review_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MdqIimDTR-o4"
   },
   "source": [
    "# üìùüè∑Ô∏è 2. Text classification\n",
    "We are going to look at some Amazon reviews and classify them into positive or negative.\n",
    "\n",
    "## From text to numbers\n",
    "Going from text to numeric data is very easy. Let's take a look at how we can do this. We'll start by separating out our X and Y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8KMW79DR-o4"
   },
   "outputs": [],
   "source": [
    "X_text = data['review_text']\n",
    "Y = data['positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeMNGKwRR-o7"
   },
   "source": [
    "Next, we will turn `X_text` into just `X` -- a numeric representation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1ObfFn7R-o7"
   },
   "outputs": [],
   "source": [
    "# Create a vectorizer that will track text as binary features\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "binary_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = binary_vectorizer.transform(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QnBSkNCoR-o9"
   },
   "source": [
    "## Modeling\n",
    "We have a ton of features, let's use them in some different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G4UUw36HR-o-"
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation\n",
    "accs = cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average accuracy rounded to three decimal points\n",
    "print(f\"The accuracy of our classifier is {np.mean(accs):3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFQ8aIUgR-pA"
   },
   "source": [
    "Let's try using full counts instead of a binary representation. I've just copy and pasted what is above and removed the `binary=True` from the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jsog1X6jR-pB"
   },
   "outputs": [],
   "source": [
    "# Create a vectorizer that will track text as counted features\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "count_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = count_vectorizer.transform(X_text)\n",
    "\n",
    "# Create a model\n",
    "logistic_regression = LogisticRegression(max_iter=100000)\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation accuracy\n",
    "accs = cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average AUC rounded to three decimal points\n",
    "print(f\"The accuracy of our classifier is {np.mean(accs):3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_3triXMR-pD"
   },
   "source": [
    "Let's try using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WB63_PIR-pD"
   },
   "outputs": [],
   "source": [
    "# Create a vectorizer that will track text as binary features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "tfidf_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = tfidf_vectorizer.transform(X_text)\n",
    "\n",
    "# Create a model\n",
    "logistic_regression = LogisticRegression(max_iter=100000)\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation accuracy\n",
    "accs = cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average AUC rounded to three decimal points\n",
    "print(f\"The accuracy of our classifier is {np.mean(accs):3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VeQBSq_yR-pH"
   },
   "source": [
    "# üîçüîß 3. Feature Engineering\n",
    "\n",
    "At the start of this class, we explored two ways of dealing with categorical data: binarizing and numerical scaling. I would like to show how to do these two things in Python. We will use the same simple 5 record data from class.\n",
    "\n",
    "Go here to get the data: http://bit.ly/someCategoricalData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9wH0oVwR-pH"
   },
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'Minutes': [100, 220, 500, 335, 450],\n",
    "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Male'],\n",
    "    'Marital': ['Single', 'Married', 'Divorced', 'Single', 'Married'],\n",
    "    'Satisfaction': ['Low', 'Very Low', 'High', 'Neutral', 'Very High'],\n",
    "    'Churn': [0, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Jt51T9gR-pL"
   },
   "source": [
    "## Binarizing\n",
    "Get a list of features you want to binarize, go through each feature and create new features for each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6Nk_S_qR-pM"
   },
   "outputs": [],
   "source": [
    "features_to_binarize = [\"Gender\", \"Marital\"]\n",
    "\n",
    "# Get dummies for the desired columns and drop the first category for each\n",
    "data_dummies = pd.get_dummies(data[features_to_binarize], drop_first=True)\n",
    "\n",
    "# Drop the original columns from the data\n",
    "data = data.drop(features_to_binarize, axis=1)\n",
    "\n",
    "# Concatenate the original data and the dummies\n",
    "data = pd.concat([data, data_dummies], axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQL_xYHTR-pQ"
   },
   "source": [
    "## Numeric scaling\n",
    "We can also replace text levels with some numeric mapping we create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OgO7VQTR-pS"
   },
   "outputs": [],
   "source": [
    "data['Satisfaction'] = data['Satisfaction'].replace(['Very Low', 'Low', 'Neutral', 'High', 'Very High'], \n",
    "                                                    [-2, -1, 0, 1, 2])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new features\n",
    "\n",
    "When modeling certain datasets, linear relationships between the features and the target variable might not capture the underlying patterns effectively. By squaring a feature, we introduce a form of non-linearity to our model.\n",
    "- The 'minutes_squared' feature, for instance, can help in situations where the effect of minutes on the target variable accelerates or decelerates. Think of it as a way to capture the idea that \"every additional minute has a larger (or smaller) effect than the previous one.\"\n",
    "- For example, in scenarios like battery discharge, the first few minutes might not have a significant effect, but as time progresses, every additional minute might have a more pronounced effect. By squaring the 'minutes' feature, we can help our model learn such patterns.\n",
    "- Remember that it's crucial to validate the performance of the model with this new feature. It's possible that for some datasets, introducing such non-linearity might not be beneficial and could even overcomplicate the model. Always rely on cross-validation or a test set to evaluate the impact of introducing new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Minutes_squared'] = data['Minutes'] ** 2\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is an important part of creating a machine learning model. It's a process that requires a lot of creativity and domain knowledge. It's also a process that can be automated to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. More text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "N-grams are contiguous sequences of n items (words, letters) from a given sample of text. Beyond just individual words (1-grams or unigrams), using 2-grams, 3-grams, etc., helps in capturing phrases and the context in which words appear. For instance, \"not good\" as a 2-gram has a different sentiment than the individual words \"not\" and \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"files/books.csv\", quotechar=\"\\\"\", escapechar=\"\\\\\")\n",
    "text_data = data['review_text']\n",
    "\n",
    "# Using 1-grams\n",
    "unigram_vectorizer = CountVectorizer()\n",
    "X_unigrams = unigram_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Using 2-grams\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_bigrams = bigram_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# notice that the number of columns greatly increased\n",
    "print(X_unigrams.shape)\n",
    "print(X_bigrams.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging classifies words into their parts of speech (like nouns, verbs, adjectives). Understanding the grammatical structure can be beneficial in various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download these two first\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sample_text = text_data.iloc[0]\n",
    "tokens = nltk.word_tokenize(sample_text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(sample_text)\n",
    "for token, pos in pos_tags:\n",
    "    print(f\"{token} - {pos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Complexity & Readability\n",
    "Readability scores indicate how difficult a reading passage is to understand. For instance, the Flesch-Kincaid score can be used to gauge the complexity of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install textstat\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "readability_score = flesch_reading_ease(sample_text)\n",
    "print(f\"Text: {sample_text} \\nReadability score: {readability_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "Both techniques reduce words to their base or root form. \n",
    "- Stemming can be more crude and cut off prefixes/suffixes\n",
    "- Lemmatization ensures the root word is meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sample_word = [\"running\", \"ran\", \"runs\", \"runner\"]\n",
    "for word in sample_word:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    print(f\"The word {word} stemmed is {stemmed_word} and lemmatized is {lemmatized_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "print(f\"Filtered tokens: {filtered_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Challenge\n",
    "\n",
    "Go to the data set in the beggining of this lecture. \n",
    "\n",
    "The challenge is to build a better predictive model.\n",
    "Some things you can do are the following\n",
    "- use n-grams instead of 1-grams\n",
    "- remove stopwords\n",
    "- lower case\n",
    "- use a stemmer \n",
    "- remove punctuation\n",
    "- use a different machine learning model\n",
    "- ....anything else you might imagine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Feature engineering, text mining, and text classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
